#Relevant packages:
#install.packages("mgcv")
#install.packages("randomForest")
#install.packages("xgboost")
#install.packages("neuralnet")
#install.packages("nnet")

#Let's clear our workspace:
rm(list = ls())

#Let's read in the data set again...
data_las <- read.csv("LasVegasTripAdvisorReviews.csv")
#...and las before et's get rid of the user country and the hotel name just for keeping it simple:
data_las <- data_las[,-c(1,14)] #Eliminates the 1st and 14th column

#As before, W=we split the data into a so-called "training" and a so-called "test" set
set.seed(42) #So that we can reproduce, we set the random "seed"
data_las <- data_las[sample(1:504),] 
data_las_train <- data_las[1:336,]
data_las_test  <- data_las[337:504,]
y_test <- data_las_test$Score

#Recall that in this case, the LASSO resulted in a better performing model than
#the plain linear regression.  Let's run it again for comparison:

#OLS MODEL
m4 <- lm(Score ~ Free.internet + Pool + Traveler.type + Spa + Hotel.stars + Gym + Casino, data = data_las_train)
yhat4  <- predict(m4,data_las_test)
fits_test = cor(yhat4,y_test)^2 

#LASSO MODEL

library(glmnet)
X_train <- data.matrix(data_las_train[,c(1:3,5:13)])
Y_train <- data_las_train[,4]
X_test <- data.matrix(data_las_test[,c(1:3,5:13)])
Y_test <- data_las_test[,4]
cv_lasso <- cv.glmnet(X_train, Y_train, family="gaussian",k=10) 
lambda <- cv_lasso$lambda.min
lasso_best <- glmnet(X_train, Y_train, family = "gaussian", alpha = 1, standardize = TRUE)
cor(predict(lasso_best,X_test,s=lambda),y_test)^2
#And let's store the R^2 in a variable:
fits_test <- c(fits_test,cor(predict(lasso_best,X_test,s=lambda),y_test)^2)

#We will now run a variety of machine learning methods.  Here the syntax isn't important, 
#as likely you will see produced outcome anyway:

#GENERALIZED ADDITIVE MODEL

#Let's modify the daya and add the fraction of helpfulreviews and the total number:
data_las$help <- data_las$Helpful.votes/data_las$Nr..reviews
data_las_train <- data_las[1:336,]
data_las_test  <- data_las[337:504,]

#And let's run a GAM where we use non-linear functions for help and Hotel.stars, where the degree of non-linearity is determined by cross validation
library(mgcv)
gam1 <- gam(Score ~ s(help, bs='cr') + Free.internet + Pool + Traveler.type + Spa + s(Hotel.stars, k=2) + Gym + Casino, family=gaussian, data=data_las_train)
summary(gam1)
plot(gam1, scale=0)
cor(predict(gam1,newdata = data_las_test),y_test)^2
fits_test <- c(fits_test,cor(predict(gam1,newdata = data_las_test),y_test)^2)
#So we see a clear improvement.

plot(fits_test, type="o", lwd=3, col="red")

#RANDOM FOREST

library(randomForest)
set.seed(123)
rf1 <- randomForest(Score ~ ., data=data_las_train)
#rf1 <- randomForest(Score ~ ., ntree=500, mtry=ncol(Score)/3, data=data_las_train)
summary(rf1)
cor(predict(rf1, data_las_test),y_test)^2
fits_test <- c(fits_test,cor(predict(rf1, data_las_test),y_test)^2)
plot(fits_test, type="o", lwd=3, col="red")
#Even more of an improvement

#GRADIENT BOOSTING

library(xgboost)
parm <- list(nthread=2, max_depth=2, eta=0.10)
bt1 <- xgboost(parm, data=X_train, label=Y_train, verbose=2, nrounds=10)
cor(predict(bt1, X_test),y_test)^2
fits_test <- c(fits_test,cor(predict(bt1, X_test),y_test)^2)
plot(fits_test, type="o", lwd=3, col="red")
#and even more of an improvmenet

#Let's look which variables enter the model:
imp <- xgb.importance(feature_names=colnames(X_train), model=bt1)
xgb.plot.importance(imp, rel_to_first = TRUE, xlab = "Relative importance")
#Note that this is in stark contrast to the linear model where we dropped the reviewer attributes...
